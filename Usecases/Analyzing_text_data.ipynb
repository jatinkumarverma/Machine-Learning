{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeMVx1E9XwhpHXnZNB3Iby"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Text Data\n",
        "\n",
        "## by Dr. Jatin Kumar Verma"
      ],
      "metadata": {
        "id": "VI8yoCeYfxYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data using tokenization"
      ],
      "metadata": {
        "id": "Q-mC6qxOgCKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\"\n",
        "\n",
        "!pip install -q nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HykPy5MjimVo",
        "outputId": "09239d03-5d72-4fa3-ff3f-2afee641cdde"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\"\n",
        "\n",
        "sent_tokenize_list = sent_tokenize(text)\n",
        "print(sent_tokenize_list)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "print (word_tokenize(text))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ84QBU-gYSy",
        "outputId": "330e4b8c-772a-4f6e-f574-a9321f2f181e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n",
            "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming text data"
      ],
      "metadata": {
        "id": "faq4OMaZluwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "DO0p7xq3l7e6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qq1VtfjMmJem"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
        "\n",
        "stemmer_porter = PorterStemmer()\n",
        "stemmer_lancaster = LancasterStemmer()\n",
        "stemmer_snowball = SnowballStemmer('english')\n",
        "\n",
        "formatted_row = '{:>16}' * (len(stemmers) + 1)\n",
        "print ('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n",
        "\n",
        "for word in words:\n",
        "   stemmed_words = [stemmer_porter.stem(word),\n",
        "    stemmer_lancaster.stem(word),\n",
        "    stemmer_snowball.stem(word)]\n",
        "   print (formatted_row.format(word, *stemmed_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jcI1GMdmdvr",
        "outputId": "5acd865a-5a9a-4fc0-b8ce-010ef17f4044"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "             WORD          PORTER       LANCASTER        SNOWBALL \n",
            "\n",
            "           table            tabl            tabl            tabl\n",
            "        probably         probabl            prob         probabl\n",
            "          wolves            wolv            wolv            wolv\n",
            "         playing            play            play            play\n",
            "              is              is              is              is\n",
            "             dog             dog             dog             dog\n",
            "             the             the             the             the\n",
            "         beaches           beach           beach           beach\n",
            "        grounded          ground          ground          ground\n",
            "          dreamt          dreamt          dreamt          dreamt\n",
            "        envision           envis           envid           envis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting text to its base form using lemmatization\n"
      ],
      "metadata": {
        "id": "DvG5PQaAnT5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']\n",
        "\n",
        "lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
        "\n",
        "lemmatizer_wordnet = WordNetLemmatizer()\n",
        "\n",
        "formatted_row = '{:>24}' * (len(lemmatizers) + 1)\n",
        "print ('\\n', formatted_row.format('WORD', *lemmatizers), '\\n')\n",
        "\n",
        "for word in words:\n",
        "   lemmatized_words = [lemmatizer_wordnet.lemmatize(word,\n",
        "    pos='n'),\n",
        "     lemmatizer_wordnet.lemmatize(word, pos='v')]\n",
        "   print (formatted_row.format(word, *lemmatized_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISxPtwg_nctg",
        "outputId": "acbe996d-e530-4700-e0c8-ee3ae20335ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
            "\n",
            "                   table                   table                   table\n",
            "                probably                probably                probably\n",
            "                  wolves                    wolf                  wolves\n",
            "                 playing                 playing                    play\n",
            "                      is                      is                      be\n",
            "                     dog                     dog                     dog\n",
            "                     the                     the                     the\n",
            "                 beaches                   beach                   beach\n",
            "                grounded                grounded                  ground\n",
            "                  dreamt                  dreamt                   dream\n",
            "                envision                envision                envision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividing text using chunking"
      ],
      "metadata": {
        "id": "ViSFcJLoow84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "import numpy as np\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def splitter(data, num_words):\n",
        "   words = data.split(' ')\n",
        "   output = []\n",
        "\n",
        "   cur_count = 0\n",
        "   cur_words = []\n",
        "\n",
        "   for word in words:\n",
        "     cur_words.append(word)\n",
        "     cur_count += 1\n",
        "\n",
        "   if cur_count == num_words:\n",
        "         output.append(' '.join(cur_words))\n",
        "         cur_words = []\n",
        "         cur_count = 0\n",
        "\n",
        "   output.append(' '.join(cur_words) )\n",
        "   return output\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    data = ' '.join(brown.words()[:10000])\n",
        "\n",
        "\n",
        "    num_words = 1700\n",
        "\n",
        "    chunks = []\n",
        "    counter = 0\n",
        "\n",
        "    text_chunks = splitter(data, num_words)\n",
        "    print (\"Number of text chunks =\", len(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHoUNXpTo98f",
        "outputId": "8b9f55f3-e168-4bf7-9cb3-3eba6c19c396"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of text chunks = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a text classifier"
      ],
      "metadata": {
        "id": "WkBQq-dNq666"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf means term frequency & idf means inverse document frequency\n"
      ],
      "metadata": {
        "id": "1XbUvF5fulGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', 'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', 'sci.space': 'Space'}\n",
        "\n",
        "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=7)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train_termcounts = vectorizer.fit_transform(training_data.data)\n",
        "print(\"\\nDimensions of training data:\", X_train_termcounts.shape)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "input_data = [\n",
        "    \"The curveballs of right handed pitchers tend to curve to the left\",\n",
        "        \"Caesar cipher is an ancient form of encryption\",\n",
        "            \"This two-wheeler is really good on slippery roads\"\n",
        "            ]\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)\n",
        "\n",
        "classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)\n",
        "\n",
        "X_input_termcounts = vectorizer.transform(input_data)\n",
        "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)\n",
        "\n",
        "predicted_categories = classifier.predict(X_input_tfidf)\n",
        "\n",
        "for sentence, category in zip(input_data, predicted_categories):\n",
        "       print('\\nInput:', sentence, '\\nPredicted category:', category_map[training_data.target_names[category]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXj_m_ggsY2U",
        "outputId": "73042619-002d-4906-b8fb-6ca0ee2d50ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensions of training data: (2968, 40605)\n",
            "\n",
            "Input: The curveballs of right handed pitchers tend to curve to the left \n",
            "Predicted category: Baseball\n",
            "\n",
            "Input: Caesar cipher is an ancient form of encryption \n",
            "Predicted category: Cryptography\n",
            "\n",
            "Input: This two-wheeler is really good on slippery roads \n",
            "Predicted category: Motorcycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing the sentiment of a sentence"
      ],
      "metadata": {
        "id": "Ye8GQEN0uLiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "import nltk.classify.util\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "\n",
        "def extract_features(word_list):\n",
        "    return dict([(word, True) for word in word_list])\n",
        "\n",
        "\n",
        "positive_fileids = movie_reviews.fileids('pos')\n",
        "negative_fileids = movie_reviews.fileids('neg')\n",
        "\n",
        "\n",
        "features_positive = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in positive_fileids]\n",
        "features_negative = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in negative_fileids]\n",
        "\n",
        "\n",
        "threshold_factor = 0.8\n",
        "threshold_positive = int(threshold_factor * len(features_positive))\n",
        "threshold_negative = int(threshold_factor * len(features_negative))\n",
        "\n",
        "\n",
        "features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]\n",
        "features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]\n",
        "\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(features_train)\n",
        "\n",
        "\n",
        "print(\"\\nAccuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n",
        "\n",
        "\n",
        "print(\"\\nTop 10 most informative words:\")\n",
        "for item in classifier.most_informative_features()[:10]:\n",
        "        print(item[0])\n",
        "\n",
        "\n",
        "input_reviews = [\n",
        "            \"It is an amazing movie\",\n",
        "                \"This is a dull movie. I would never recommend it to anyone.\",\n",
        "                    \"The cinematography is pretty great in this movie\",\n",
        "                        \"The direction was terrible and the story was all over the place\"\n",
        "                        ]\n",
        "\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for review in input_reviews:\n",
        "                            print(\"\\nReview:\", review)\n",
        "                            probdist = classifier.prob_classify(extract_features(review.split()))\n",
        "                            pred_sentiment = probdist.max()\n",
        "                            print(\"Predicted sentiment:\", pred_sentiment)\n",
        "                            print(\"Probability:\", round(probdist.prob(pred_sentiment), 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9N3NatzvJkU",
        "outputId": "52b737b6-830e-47f2-deed-5bbe77d073a4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy of the classifier: 0.735\n",
            "\n",
            "Top 10 most informative words:\n",
            "outstanding\n",
            "insulting\n",
            "vulnerable\n",
            "ludicrous\n",
            "uninvolving\n",
            "astounding\n",
            "avoids\n",
            "fascination\n",
            "affecting\n",
            "animators\n",
            "\n",
            "Predictions:\n",
            "\n",
            "Review: It is an amazing movie\n",
            "Predicted sentiment: Positive\n",
            "Probability: 0.61\n",
            "\n",
            "Review: This is a dull movie. I would never recommend it to anyone.\n",
            "Predicted sentiment: Negative\n",
            "Probability: 0.77\n",
            "\n",
            "Review: The cinematography is pretty great in this movie\n",
            "Predicted sentiment: Positive\n",
            "Probability: 0.67\n",
            "\n",
            "Review: The direction was terrible and the story was all over the place\n",
            "Predicted sentiment: Negative\n",
            "Probability: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying patterns in text using topic modeling"
      ],
      "metadata": {
        "id": "ZA2M6-gPyyRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from gensim import models, corpora\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def load_data(input_file):\n",
        "   data = []\n",
        "   with open(input_file, 'r') as f:\n",
        "     for line in f.readlines():\n",
        "      data.append(line[:-1])\n",
        "   return data\n",
        "\n",
        "class Preprocessor(object):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "      self.tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "      self.stop_words_english = stopwords.words('english')\n",
        "\n",
        "      self.stemmer = SnowballStemmer('english')\n",
        "\n",
        "    def process(self, input_text):\n",
        "\n",
        "         tokens = self.tokenizer.tokenize(input_text.lower())\n",
        "         tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
        "\n",
        "         tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
        "\n",
        "\n",
        "         return tokens_stemmed\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "            input_file = 'data_modeling.txt'\n",
        "\n",
        "            data = load_data(input_file)\n",
        "\n",
        "\n",
        "            preprocessor = Preprocessor()\n",
        "\n",
        "\n",
        "            processed_tokens = [preprocessor.process(x) for x in data]\n",
        "\n",
        "\n",
        "\n",
        "            dict_tokens = corpora.Dictionary(processed_tokens)\n",
        "\n",
        "\n",
        "            corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
        "\n",
        "            num_topics = 2\n",
        "            num_words = 4\n",
        "\n",
        "            ldamodel = models.ldamodel.LdaModel(corpus,  num_topics=num_topics, id2word=dict_tokens,  passes=25)\n",
        "\n",
        "            print (\"Most contributing words to the topics:\")\n",
        "            for item in ldamodel.print_topics(num_topics=num_topics,\n",
        "               num_words=num_words):\n",
        "                print( \"\\nTopic\", item[0], \"==>\", item[1])"
      ],
      "metadata": {
        "id": "WiAL2N3Oy2n_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}